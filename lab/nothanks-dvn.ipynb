{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "me (1), other players (N-1), merged (1) //, open (1)\n",
    "card: (N+1)x33\n",
    "pot: (N+1)x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ipypb import irange\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DVN(nn.Module):\n",
    "    def __init__(self, n=5):\n",
    "        super(DVN, self).__init__()\n",
    "        \n",
    "        sz1 = (n+1)*34\n",
    "        sz2 = (n+1)*30\n",
    "        sz3 = (n+1)*26\n",
    "        sz4 = (n+1)*22\n",
    "\n",
    "        self.ds1 = nn.Linear(sz1, sz2);\n",
    "        self.ds2 = nn.Linear(sz2, sz3);\n",
    "        self.ds3 = nn.Linear(sz3, sz4);\n",
    "        self.ds4 = nn.Linear(sz4, 1, bias=False)\n",
    "        \n",
    "        nn.init.xavier_normal_(self.ds1.weight)\n",
    "        nn.init.xavier_normal_(self.ds2.weight)\n",
    "        nn.init.xavier_normal_(self.ds3.weight)\n",
    "        nn.init.xavier_normal_(self.ds4.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.ds1(x))\n",
    "        x = F.relu(self.ds2(x))\n",
    "        x = F.relu(self.ds3(x))\n",
    "        return self.ds4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state: players (N) + merged (1) x token (1) + card (33)  \n",
    "\n",
    "N = 5\n",
    "\n",
    "def mutate(state, pot, action):\n",
    "    state, pot = state.copy(), pot.copy()\n",
    "    if state[0,0] == 0:\n",
    "        action = False\n",
    "    if action:\n",
    "        state[[0,-1],0] -= 1\n",
    "        pot[0] += 1\n",
    "    else:\n",
    "        state[0] += pot\n",
    "        state[-1] += pot\n",
    "        pot *= 0\n",
    "    return state, pot\n",
    "        \n",
    "def next_turn(state):\n",
    "    return np.r_[state[1:-1], state[0:1], state[-1:]]\n",
    "\n",
    "def to_tensor(state):\n",
    "    return torch.from_numpy(state.reshape((-1, (N+1)*34))).float().to(device)\n",
    "\n",
    "def evaluate(net, state):\n",
    "    net.eval()\n",
    "    return net(to_tensor(state)).data.cpu().numpy()[0,0]\n",
    "\n",
    "def get_score(state):\n",
    "    out = state[0,0]\n",
    "    if state[0,1] == 1: \n",
    "        out -= 3\n",
    "    for i in range(2, 34):\n",
    "        if state[0,i] == 1 and state[0,i-1] == 0:\n",
    "            out -= i + 2\n",
    "    return out\n",
    "    \n",
    "def spawn_player(value_net):\n",
    "    def player(state, pot):\n",
    "        if state[0,0] == 0: \n",
    "            return False\n",
    "        sT = mutate(state, pot, True)[0]\n",
    "        sF = mutate(state, pot, False)[0]\n",
    "        vT = evaluate(value_net, sT)\n",
    "        vF = evaluate(value_net, sF)\n",
    "        \n",
    "        # print(vT, vF)\n",
    "        return vT > vF\n",
    "    return player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vn = DVN(N).to(device)\n",
    "# vn.load_state_dict(torch.load('m.mdl'))\n",
    "players = [spawn_player(vn) for _ in range(N)]\n",
    "\n",
    "# vns = [DVN(N).to(device) for _ in range(N)]\n",
    "# players = [spawn_player(vn) for vn in vns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gym(players):\n",
    "    card = np.arange(33) + 3\n",
    "    \n",
    "    deck = card.copy()\n",
    "    np.random.shuffle(deck)\n",
    "    deck = list(deck[:24])\n",
    "    def draw():\n",
    "        return np.r_[0, (card.copy() == deck.pop()).astype(int)]\n",
    "\n",
    "    n = len(players)\n",
    "    state = np.r_[[np.r_[11, np.zeros((33,), int)] for _ in range(n)]]\n",
    "    state = np.r_[state, state.sum(axis=0).reshape((1, -1))]\n",
    "    \n",
    "    turn = np.random.randint(n)\n",
    "    replay = []\n",
    "    # non-terminal: is_terminal, id, state_before, state_after, reward\n",
    "    # terminal: is_terminal, id, state, score, place\n",
    "    \n",
    "    while len(deck) > 0:\n",
    "        pot = draw()\n",
    "        while True:\n",
    "            nothanks = players[turn](state, pot)\n",
    "\n",
    "            state_, pot_ = mutate(state, pot, nothanks)\n",
    "            reward = get_score(state_) - get_score(state)\n",
    "            replay.append((False, turn, state, state_, reward))\n",
    "            if state[0,0] > 0:\n",
    "                state_f, pot_f = mutate(state, pot, not nothanks)\n",
    "                reward_f = get_score(state_f) - get_score(state)\n",
    "                replay.append((False, turn, state, state_f, reward_f))            \n",
    "            \n",
    "            if nothanks:\n",
    "                turn = (turn + 1)%n\n",
    "                state = next_turn(state_)\n",
    "                pot = pot_\n",
    "            else:\n",
    "                state = state_\n",
    "                break\n",
    "                \n",
    "    scores = []\n",
    "    for _ in range(n):\n",
    "        scores.append(-get_score(state))\n",
    "        state = next_turn(state)\n",
    "    scores.sort()\n",
    "    for _ in range(n):\n",
    "        score = get_score(state)\n",
    "        replay.append((True, turn, state, score, scores.index(-score)))\n",
    "        state = next_turn(state)\n",
    "        turn = (turn + 1)%n\n",
    "      \n",
    "    return replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(vn, replay, gamma=0.95, batch_size=64, place_value=[0, 0, 0, 0, 0], epoch=10):\n",
    "    vn.train()\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    optimizer = optim.Adam(vn.parameters())\n",
    "    \n",
    "    n_replay = len(replay)\n",
    "    for _ in range(epoch):\n",
    "        argseq = np.arange(n_replay)\n",
    "        np.random.shuffle(argseq)\n",
    "        for i in irange(0, n_replay, batch_size):\n",
    "            x = []\n",
    "            y = []\n",
    "            if argseq[i:].size < batch_size:\n",
    "                break\n",
    "            for j in range(batch_size):\n",
    "                is_terminal, _, state, a, b = replay[argseq[i+j]]\n",
    "                x.append(state.flatten())\n",
    "                if is_terminal:\n",
    "                    y.append(a + place_value[b])\n",
    "                else:\n",
    "                    state_after = a\n",
    "                    reward = b\n",
    "                    y.append(reward + evaluate(vn, state_after)*gamma)\n",
    "\n",
    "            x = to_tensor(np.r_[x])\n",
    "            y = torch.from_numpy(np.r_[y].reshape((-1,1))).float().to(device)\n",
    "            prediction = vn(x)\n",
    "            loss = criterion(prediction, target=y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lineage: 1023\n",
      "  player 4: #3 (-35): 1 - [15, 21]\n",
      "  player 0: #1 (0): 0 - []\n",
      "  player 1: #5 (-87): 54 - [3, 4, 5, 9, 11, 12, 13, 16, 20, 23, 24, 25, 26, 28, 29, 31, 32, 33]\n",
      "  player 2: #2 (-34): 0 - [7, 10, 17]\n",
      "  player 3: #3 (-35): 0 - [35]\n",
      "total: -191\n",
      "loss: 6.3656134605407715\n"
     ]
    }
   ],
   "source": [
    "replay = []\n",
    "for lineage in range(1024):\n",
    "    for _ in irange(8):\n",
    "        replay += gym(players)\n",
    "    replay = replay[-32*256:]\n",
    "    loss = train(vn, replay, batch_size=64, epoch=1)\n",
    "    torch.save(vn.state_dict(), 'm.mdl')\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    total = 0\n",
    "    print(f'lineage: {lineage}')\n",
    "    for _, idx, state, score, place in gym(players)[-N:]:\n",
    "        total += score\n",
    "        print(f'  player {idx}: #{place+1} ({score}): {state[0,0]} - [{\", \".join([f\"{i+2}\" for i in range(1,34) if state[0,i] == 1])}]')\n",
    "    print(f'total: {total}')\n",
    "    print(f'loss:', loss.data.cpu().numpy() + 0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "replay = gym(players)\n",
    "for _, idx, state, score, place in replay[-N:]:\n",
    "    total += score\n",
    "    print(f'  player {idx}: #{place+1} ({score}): {state[0,0]} - [{\", \".join([f\"{i+2}\" for i in range(1,34) if state[0,i] == 1])}]')\n",
    "print(f'total: {total}')\n",
    "replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lineage: 1023\n",
      "  player 1: #1 (-14): 1 - [15]\n",
      "  player 2: #5 (-121): 52 - [4, 5, 6, 9, 11, 12, 16, 20, 24, 26, 27, 29, 34]\n",
      "  player 3: #4 (-114): 1 - [23, 25, 32, 35]\n",
      "  player 4: #2 (-29): 1 - [3, 10, 17]\n",
      "  player 0: #3 (-48): 0 - [8, 18, 22]\n",
      "total: -326\n",
      "loss: 4.929439067840576\n"
     ]
    }
   ],
   "source": [
    "for lineage in range(1024):\n",
    "    for _ in irange(8):\n",
    "        replay += gym(players)\n",
    "    replay = replay[-32*256:]\n",
    "    loss = train(vn, replay, batch_size=64, epoch=1)\n",
    "    torch.save(vn.state_dict(), 'm.mdl')\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    total = 0\n",
    "    print(f'lineage: {lineage}')\n",
    "    for _, idx, state, score, place in gym(players)[-N:]:\n",
    "        total += score\n",
    "        print(f'  player {idx}: #{place+1} ({score}): {state[0,0]} - [{\", \".join([f\"{i+2}\" for i in range(1,34) if state[0,i] == 1])}]')\n",
    "    print(f'total: {total}')\n",
    "    print(f'loss:', loss.data.cpu().numpy() + 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lineage: 1023\n",
      "  player 4: #2 (4): 14 - [10]\n",
      "  player 0: #3 (-10): 12 - [22]\n",
      "  player 1: #1 (10): 10 - []\n",
      "  player 2: #5 (-198): 9 - [3, 4, 7, 12, 13, 15, 20, 26, 28, 30, 32, 34, 35]\n",
      "  player 3: #4 (-155): 10 - [6, 8, 11, 14, 18, 21, 23, 31, 33]\n",
      "total: -349\n",
      "loss: 6.5375471115112305\n"
     ]
    }
   ],
   "source": [
    "for lineage in range(1024):\n",
    "    for _ in irange(8):\n",
    "        replay += gym(players)\n",
    "    replay = replay[-32*256:]\n",
    "    loss = train(vn, replay, batch_size=64, epoch=1)\n",
    "    torch.save(vn.state_dict(), 'm.mdl')\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    total = 0\n",
    "    print(f'lineage: {lineage}')\n",
    "    for _, idx, state, score, place in gym(players)[-N:]:\n",
    "        total += score\n",
    "        print(f'  player {idx}: #{place+1} ({score}): {state[\n",
    "0,0]} - [{\", \".join([f\"{i+2}\" for i in range(1,34) if state[0,i] == 1])}]')\n",
    "    print(f'total: {total}')\n",
    "    print(f'loss:', loss.data.cpu().numpy() + 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lineage: 10394\n",
      "  player 1: #3 (-42): 3 - [21, 24, 25, 26]\n",
      "  player 2: #1 (-12): 1 - [5, 8]\n",
      "  player 3: #5 (-91): 3 - [12, 19, 28, 35]\n",
      "  player 4: #2 (-39): 47 - [4, 6, 7, 17, 27, 32, 33]\n",
      "  player 0: #4 (-56): 1 - [3, 9, 10, 11, 16, 29, 30]\n",
      "total: -240\n",
      "loss: 6.625244140625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:0; max-width:15ex; vertical-align:middle; text-align:right\"></span>\n",
       "<progress style=\"width:60ex\" max=\"8\" value=\"8\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">8/8</span>\n",
       "<span class=\"Time-label\">[00:01<00:00, 0.07s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       " [████████████████████████████████████████████████████████████] 8/8 [00:01<00:00, 0.07s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:0; max-width:15ex; vertical-align:middle; text-align:right\"></span>\n",
       "<progress style=\"width:60ex\" max=\"128\" value=\"97\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>76%</strong></span>\n",
       "<span class=\"Iteration-label\">97/128</span>\n",
       "<span class=\"Time-label\">[00:02<00:00, 0.02s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       " [█████████████████████████████████████████████###############] 97/128 [00:02<00:00, 0.02s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4f907912ec9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mreplay\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mreplay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreplay\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'm.mdl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-6dd0c2fcaac7>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(vn, replay, gamma, batch_size, place_value, epoch)\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sonjh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sonjh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    105\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for lineage in range(1024*64):\n",
    "    for _ in irange(8):\n",
    "        replay += gym(players)\n",
    "    replay = replay[-32*256:]\n",
    "    loss = train(vn, replay, batch_size=64, epoch=1)\n",
    "    torch.save(vn.state_dict(), 'm.mdl')\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    total = 0\n",
    "    print(f'lineage: {lineage}')\n",
    "    for _, idx, state, score, place in gym(players)[-N:]:\n",
    "        total += score\n",
    "        print(f'  player {idx}: #{place+1} ({score}): {state[0,0]} - [{\", \".join([f\"{i+2}\" for i in range(1,34) if state[0,i] == 1])}]')\n",
    "    print(f'total: {total}')\n",
    "    print(f'loss:', loss.data.cpu().numpy() + 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
